{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "8155dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SUB_ID</th>\n",
       "      <th>X</th>\n",
       "      <th>subject</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>anat_cnr</th>\n",
       "      <th>anat_efc</th>\n",
       "      <th>anat_fber</th>\n",
       "      <th>anat_fwhm</th>\n",
       "      <th>anat_qi1</th>\n",
       "      <th>anat_snr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>50002</td>\n",
       "      <td>1</td>\n",
       "      <td>50002</td>\n",
       "      <td>PITT</td>\n",
       "      <td>no_filename</td>\n",
       "      <td>1</td>\n",
       "      <td>10.201539</td>\n",
       "      <td>1.194664</td>\n",
       "      <td>16.223458</td>\n",
       "      <td>3.878000</td>\n",
       "      <td>0.152711</td>\n",
       "      <td>12.072452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050003</td>\n",
       "      <td>1</td>\n",
       "      <td>7.165701</td>\n",
       "      <td>1.126752</td>\n",
       "      <td>10.460008</td>\n",
       "      <td>4.282238</td>\n",
       "      <td>0.161716</td>\n",
       "      <td>9.241155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050004</td>\n",
       "      <td>1</td>\n",
       "      <td>7.698144</td>\n",
       "      <td>1.226218</td>\n",
       "      <td>9.725750</td>\n",
       "      <td>3.881684</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>9.323463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050005</td>\n",
       "      <td>1</td>\n",
       "      <td>9.071807</td>\n",
       "      <td>1.256278</td>\n",
       "      <td>11.198226</td>\n",
       "      <td>3.628667</td>\n",
       "      <td>0.119269</td>\n",
       "      <td>10.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050006</td>\n",
       "      <td>1</td>\n",
       "      <td>8.026798</td>\n",
       "      <td>1.407166</td>\n",
       "      <td>6.282055</td>\n",
       "      <td>3.674539</td>\n",
       "      <td>0.130647</td>\n",
       "      <td>10.123574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SUB_ID  X  subject SITE_ID       FILE_ID  DX_GROUP   anat_cnr  \\\n",
       "0           1   50002  1    50002    PITT   no_filename         1  10.201539   \n",
       "1           2   50003  2    50003    PITT  Pitt_0050003         1   7.165701   \n",
       "2           3   50004  3    50004    PITT  Pitt_0050004         1   7.698144   \n",
       "3           4   50005  4    50005    PITT  Pitt_0050005         1   9.071807   \n",
       "4           5   50006  5    50006    PITT  Pitt_0050006         1   8.026798   \n",
       "\n",
       "   anat_efc  anat_fber  anat_fwhm  anat_qi1   anat_snr  \n",
       "0  1.194664  16.223458   3.878000  0.152711  12.072452  \n",
       "1  1.126752  10.460008   4.282238  0.161716   9.241155  \n",
       "2  1.226218   9.725750   3.881684  0.174186   9.323463  \n",
       "3  1.256278  11.198226   3.628667  0.119269  10.814200  \n",
       "4  1.407166   6.282055   3.674539  0.130647  10.123574  "
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "df = pd.read_csv('Phenotypic_V1_0b_preprocessed1.csv')\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "f304312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data samples:\n",
      "(583, 2016)\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"features.npz\")['a']\n",
    "y = np.load(\"labels.npz\")['a']\n",
    "y = np.select([y == 1, y == 2], [0, 1], y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=69)\n",
    "\n",
    "print(\"\\nTraining data samples:\")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "6091d182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1.])\n",
      "[[ 1.21088819  2.04006818  1.41930824 ...  1.10931878  1.56545776\n",
      "   1.47208363]\n",
      " [ 0.44697634  1.24361063  0.96518331 ...  0.81550805  0.47295037\n",
      "   1.39764759]\n",
      " [-1.06991696 -0.95606267 -0.20732373 ... -2.67919839 -0.42278902\n",
      "  -1.81686175]\n",
      " ...\n",
      " [ 1.19391984  1.61272229  1.38968103 ... -1.4455497   0.27341054\n",
      "  -0.1307982 ]\n",
      " [-0.03878601  1.17716578 -0.80528464 ... -0.95932201 -0.9795447\n",
      "  -0.8050224 ]\n",
      " [ 0.88550381  0.93902838 -0.64995551 ...  0.80399731  1.46718011\n",
      "   1.6549649 ]]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#y_train =  torch.from_numpy(y_train.ravel()).float()\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "print(y_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "01ed3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train data\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "#Calling dataset class to train on X and y tensors\n",
    "train_data = TrainData(torch.FloatTensor(X_train), \n",
    "                       torch.FloatTensor(y_train))\n",
    "#Function to test data\n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "#Calling dataset class to test on X tensor\n",
    "test_data = TestData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "3c7900fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "58cdf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(2016, 256) \n",
    "        self.layer_2 = nn.Linear(256, 50)\n",
    "        self.layer_out = nn.Linear(50, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(50)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "d14974f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "BinaryClassification(\n",
      "  (layer_1): Linear(in_features=2016, out_features=256, bias=True)\n",
      "  (layer_2): Linear(in_features=256, out_features=50, bias=True)\n",
      "  (layer_out): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = BinaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "7826ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "ff9cf586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.72079 | Acc: 55.300\n",
      "Epoch 002: | Train Loss: 0.57350 | Acc: 68.100\n",
      "Epoch 003: | Train Loss: 0.50200 | Acc: 78.000\n",
      "Epoch 004: | Train Loss: 0.38745 | Acc: 86.500\n",
      "Epoch 005: | Train Loss: 0.37216 | Acc: 86.700\n",
      "Epoch 006: | Train Loss: 0.29210 | Acc: 90.800\n",
      "Epoch 007: | Train Loss: 0.28814 | Acc: 91.100\n",
      "Epoch 008: | Train Loss: 0.21501 | Acc: 94.700\n",
      "Epoch 009: | Train Loss: 0.18707 | Acc: 94.800\n",
      "Epoch 010: | Train Loss: 0.12617 | Acc: 98.100\n",
      "Epoch 011: | Train Loss: 0.11774 | Acc: 97.700\n",
      "Epoch 012: | Train Loss: 0.11431 | Acc: 97.500\n",
      "Epoch 013: | Train Loss: 0.13507 | Acc: 96.400\n",
      "Epoch 014: | Train Loss: 0.15640 | Acc: 96.800\n",
      "Epoch 015: | Train Loss: 0.16032 | Acc: 96.200\n",
      "Epoch 016: | Train Loss: 0.18632 | Acc: 92.500\n",
      "Epoch 017: | Train Loss: 0.16223 | Acc: 95.100\n",
      "Epoch 018: | Train Loss: 0.11925 | Acc: 96.500\n",
      "Epoch 019: | Train Loss: 0.08198 | Acc: 98.000\n",
      "Epoch 020: | Train Loss: 0.05249 | Acc: 100.000\n",
      "Epoch 021: | Train Loss: 0.07302 | Acc: 97.900\n",
      "Epoch 022: | Train Loss: 0.04757 | Acc: 99.400\n",
      "Epoch 023: | Train Loss: 0.06051 | Acc: 99.200\n",
      "Epoch 024: | Train Loss: 0.04877 | Acc: 99.400\n",
      "Epoch 025: | Train Loss: 0.08410 | Acc: 98.400\n",
      "Epoch 026: | Train Loss: 0.05174 | Acc: 99.100\n",
      "Epoch 027: | Train Loss: 0.03939 | Acc: 99.800\n",
      "Epoch 028: | Train Loss: 0.09135 | Acc: 96.300\n",
      "Epoch 029: | Train Loss: 0.09375 | Acc: 96.300\n",
      "Epoch 030: | Train Loss: 0.08267 | Acc: 98.500\n",
      "Epoch 031: | Train Loss: 0.05626 | Acc: 99.100\n",
      "Epoch 032: | Train Loss: 0.11123 | Acc: 98.300\n",
      "Epoch 033: | Train Loss: 0.06359 | Acc: 99.000\n",
      "Epoch 034: | Train Loss: 0.04736 | Acc: 99.300\n",
      "Epoch 035: | Train Loss: 0.03881 | Acc: 99.800\n",
      "Epoch 036: | Train Loss: 0.09603 | Acc: 94.900\n",
      "Epoch 037: | Train Loss: 0.14256 | Acc: 94.600\n",
      "Epoch 038: | Train Loss: 0.16217 | Acc: 94.100\n",
      "Epoch 039: | Train Loss: 0.11113 | Acc: 96.900\n",
      "Epoch 040: | Train Loss: 0.18710 | Acc: 93.000\n",
      "Epoch 041: | Train Loss: 0.14771 | Acc: 95.800\n",
      "Epoch 042: | Train Loss: 0.12769 | Acc: 95.800\n",
      "Epoch 043: | Train Loss: 0.14099 | Acc: 94.200\n",
      "Epoch 044: | Train Loss: 0.08635 | Acc: 97.800\n",
      "Epoch 045: | Train Loss: 0.04553 | Acc: 99.400\n",
      "Epoch 046: | Train Loss: 0.02615 | Acc: 100.000\n",
      "Epoch 047: | Train Loss: 0.02614 | Acc: 99.800\n",
      "Epoch 048: | Train Loss: 0.01586 | Acc: 100.000\n",
      "Epoch 049: | Train Loss: 0.01089 | Acc: 100.000\n",
      "Epoch 050: | Train Loss: 0.02723 | Acc: 98.600\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "0c2810fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        \n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "#print(y_pred_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "85a7cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Valid Loss: 0.01988 | Acc: 99.400\n",
      "Epoch 002: | Valid Loss: 0.01758 | Acc: 99.800\n",
      "Epoch 003: | Valid Loss: 0.01384 | Acc: 100.000\n",
      "Epoch 004: | Valid Loss: 0.00484 | Acc: 100.000\n",
      "Epoch 005: | Valid Loss: 0.00280 | Acc: 100.000\n",
      "Epoch 006: | Valid Loss: 0.00196 | Acc: 100.000\n",
      "Epoch 007: | Valid Loss: 0.00178 | Acc: 100.000\n",
      "Epoch 008: | Valid Loss: 0.00162 | Acc: 100.000\n",
      "Epoch 009: | Valid Loss: 0.00139 | Acc: 100.000\n",
      "Epoch 010: | Valid Loss: 0.00132 | Acc: 100.000\n",
      "Epoch 011: | Valid Loss: 0.00126 | Acc: 100.000\n",
      "Epoch 012: | Valid Loss: 0.00113 | Acc: 100.000\n",
      "Epoch 013: | Valid Loss: 0.00109 | Acc: 100.000\n",
      "Epoch 014: | Valid Loss: 0.00104 | Acc: 100.000\n",
      "Epoch 015: | Valid Loss: 0.00100 | Acc: 100.000\n",
      "Epoch 016: | Valid Loss: 0.00095 | Acc: 100.000\n",
      "Epoch 017: | Valid Loss: 0.00086 | Acc: 100.000\n",
      "Epoch 018: | Valid Loss: 0.00083 | Acc: 100.000\n",
      "Epoch 019: | Valid Loss: 0.00079 | Acc: 100.000\n",
      "Epoch 020: | Valid Loss: 0.00084 | Acc: 100.000\n",
      "Epoch 021: | Valid Loss: 0.00073 | Acc: 100.000\n",
      "Epoch 022: | Valid Loss: 0.00068 | Acc: 100.000\n",
      "Epoch 023: | Valid Loss: 0.00067 | Acc: 100.000\n",
      "Epoch 024: | Valid Loss: 0.00066 | Acc: 100.000\n",
      "Epoch 025: | Valid Loss: 0.00062 | Acc: 100.000\n",
      "Epoch 026: | Valid Loss: 0.00059 | Acc: 100.000\n",
      "Epoch 027: | Valid Loss: 0.00060 | Acc: 100.000\n",
      "Epoch 028: | Valid Loss: 0.00059 | Acc: 100.000\n",
      "Epoch 029: | Valid Loss: 0.00053 | Acc: 100.000\n",
      "Epoch 030: | Valid Loss: 0.00055 | Acc: 100.000\n",
      "Epoch 031: | Valid Loss: 0.00059 | Acc: 100.000\n",
      "Epoch 032: | Valid Loss: 0.00050 | Acc: 100.000\n",
      "Epoch 033: | Valid Loss: 0.00049 | Acc: 100.000\n",
      "Epoch 034: | Valid Loss: 0.00045 | Acc: 100.000\n",
      "Epoch 035: | Valid Loss: 0.00046 | Acc: 100.000\n",
      "Epoch 036: | Valid Loss: 0.00045 | Acc: 100.000\n",
      "Epoch 037: | Valid Loss: 0.00043 | Acc: 100.000\n",
      "Epoch 038: | Valid Loss: 0.00040 | Acc: 100.000\n",
      "Epoch 039: | Valid Loss: 0.00041 | Acc: 100.000\n",
      "Epoch 040: | Valid Loss: 0.00037 | Acc: 100.000\n",
      "Epoch 041: | Valid Loss: 0.00039 | Acc: 100.000\n",
      "Epoch 042: | Valid Loss: 0.00037 | Acc: 100.000\n",
      "Epoch 043: | Valid Loss: 0.00037 | Acc: 100.000\n",
      "Epoch 044: | Valid Loss: 0.00033 | Acc: 100.000\n",
      "Epoch 045: | Valid Loss: 0.00033 | Acc: 100.000\n",
      "Epoch 046: | Valid Loss: 0.00034 | Acc: 100.000\n",
      "Epoch 047: | Valid Loss: 0.00032 | Acc: 100.000\n",
      "Epoch 048: | Valid Loss: 0.00030 | Acc: 100.000\n",
      "Epoch 049: | Valid Loss: 0.00031 | Acc: 100.000\n",
      "Epoch 050: | Valid Loss: 0.00029 | Acc: 100.000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Valid Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "98d0de25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.70      0.67       132\n",
      "         1.0       0.73      0.67      0.70       156\n",
      "\n",
      "    accuracy                           0.68       288\n",
      "   macro avg       0.68      0.69      0.68       288\n",
      "weighted avg       0.69      0.68      0.68       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
